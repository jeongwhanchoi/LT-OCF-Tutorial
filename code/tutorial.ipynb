{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clone the tutorial repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "██╗  ████████╗    ██████╗  ██████╗███████╗\n",
      "██║  ╚══██╔══╝   ██╔═══██╗██╔════╝██╔════╝\n",
      "██║     ██║█████╗██║   ██║██║     █████╗  \n",
      "██║     ██║╚════╝██║   ██║██║     ██╔══╝  \n",
      "███████╗██║      ╚██████╔╝╚██████╗██║     \n",
      "╚══════╝╚═╝       ╚═════╝  ╚═════╝╚═╝     \n",
      "                                                      \n",
      "\n",
      "Current cuda device  0\n",
      "\u001b[0;30;43mloading [../data/gowalla]\u001b[0m\n",
      "810128 interactions for training\n",
      "217242 interactions for testing\n",
      "gowalla Sparsity : 0.0008396216228570436\n",
      "gowalla is ready to go\n",
      "===========config================\n",
      "{'A_n_fold': 100,\n",
      " 'A_split': False,\n",
      " 'K': 4,\n",
      " 'bigdata': False,\n",
      " 'bpr_batch_size': 2048,\n",
      " 'decay': 0.0001,\n",
      " 'dropout': 0,\n",
      " 'dual_res': False,\n",
      " 'keep_prob': 0.6,\n",
      " 'latent_dim_rec': 64,\n",
      " 'learnable_time': False,\n",
      " 'lightGCN_n_layers': 3,\n",
      " 'lr': 0.001,\n",
      " 'lr_time': 0.0001,\n",
      " 'multicore': 0,\n",
      " 'pretrain': 0,\n",
      " 'pretrained_file_name': 'ltocf',\n",
      " 'solver': 'euler',\n",
      " 'test_u_batch_size': 100,\n",
      " 'time_split': 4}\n",
      "cores for test: 32\n",
      "comment: lt-ncf\n",
      "tensorboard: 1\n",
      "LOAD: 0\n",
      "Weight path: ./checkpoints\n",
      "Test Topks: [20]\n",
      "using bpr loss\n",
      "adjoint method: False\n",
      "===========end===================\n",
      ">>SEED: 2020\n"
     ]
    }
   ],
   "source": [
    "import world\n",
    "import utils\n",
    "from world import cprint\n",
    "import register\n",
    "import torch\n",
    "import numpy as np\n",
    "from tensorboardX import SummaryWriter\n",
    "import time\n",
    "import Procedure\n",
    "from os.path import join\n",
    "# ==============================\n",
    "utils.set_seed(world.seed)\n",
    "print(\">>SEED:\", world.seed)\n",
    "# ==============================\n",
    "\n",
    "from register import dataset\n",
    "from torch import nn\n",
    "\n",
    "from dataloader import BasicDataset\n",
    "import dataloader\n",
    "# import odeblock as ode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if world.adjoint:\n",
    "    from torchdiffeq import odeint_adjoint as odeint\n",
    "else:\n",
    "    from torchdiffeq import odeint\n",
    "\n",
    "class ODEFunction(nn.Module):\n",
    "    \"\"\"\n",
    "       ## linear GCN (non-time-dependent) in ODE function\n",
    "    \"\"\"\n",
    "    def __init__(self, Graph):\n",
    "        super(ODEFunction, self).__init__()\n",
    "        self.g = Graph\n",
    "\n",
    "    def forward(self, t, x):\n",
    "        \"\"\"\n",
    "        ## linear GCN (non-time-dependent) in ODE function\n",
    "        \n",
    "        \\begin{align}\n",
    "        $ \\boldsymbol{E}_{k} = \\boldsymbol{A}\\boldsymbol{E}_{k-1} $\n",
    "        \\end{align}\n",
    "        \"\"\"\n",
    "        out = torch.sparse.mm(self.g, x)\n",
    "        return out\n",
    "\n",
    "class ODEBlock(nn.Module):\n",
    "    def __init__(self, odeFunction, solver, init_time, final_time):\n",
    "        super(ODEBlock, self).__init__()\n",
    "        self.odefunc = odeFunction\n",
    "        self.integration_time = torch.tensor([init_time,final_time]).float()\n",
    "        self.solver = solver\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.integration_time = self.integration_time.type_as(x)\n",
    "        out = odeint(func=self.odefunc, y0=x, t=self.integration_time, method=self.solver)\n",
    "        return out[1]\n",
    "\n",
    "def ODETimeSplitter(num_split, K):\n",
    "        eta = K / num_split\n",
    "        return [i*eta for i in range(1, num_split)]\n",
    "        \n",
    "def ODETimeSetter(num_split, K):\n",
    "        eta = K/ num_split\n",
    "        return [torch.tensor([i*eta], dtype=torch.float32, requires_grad=True, device='cuda') for i in range(1, num_split)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FIXED\n",
    "class LTOCF(nn.Module):\n",
    "    def __init__(self, \n",
    "                 config:dict, \n",
    "                 dataset:BasicDataset):\n",
    "        super(LTOCF, self).__init__()\n",
    "        self.config = config\n",
    "        self.dataset : dataloader.BasicDataset = dataset\n",
    "        self.__init_weight()\n",
    "        self.__init_ode()\n",
    "\n",
    "\n",
    "    def __init_weight(self):\n",
    "        self.num_users  = self.dataset.n_users\n",
    "        self.num_items  = self.dataset.m_items\n",
    "        self.latent_dim = self.config['latent_dim_rec']\n",
    "        self.n_layers = self.config['lightGCN_n_layers']\n",
    "        self.keep_prob = self.config['keep_prob']\n",
    "        self.A_split = self.config['A_split']\n",
    "        self.embedding_user = torch.nn.Embedding(\n",
    "            num_embeddings=self.num_users, embedding_dim=self.latent_dim)\n",
    "        self.embedding_item = torch.nn.Embedding(\n",
    "            num_embeddings=self.num_items, embedding_dim=self.latent_dim)\n",
    "        if self.config['pretrain'] == 0:\n",
    "            nn.init.normal_(self.embedding_user.weight, std=0.1)\n",
    "            nn.init.normal_(self.embedding_item.weight, std=0.1)\n",
    "            world.cprint('use NORMAL distribution initilizer')\n",
    "        else:\n",
    "            print('use pretarined data')\n",
    "        self.f = nn.Sigmoid()\n",
    "        self.Graph = self.dataset.getSparseGraph()\n",
    "        print(f\"lgn is already to go(dropout:{self.config['dropout']})\")\n",
    "    \n",
    "    def __init_ode(self):\n",
    "        self.time_split = self.config['time_split'] # init the number of time split\n",
    "        \n",
    "        self.odetime_splitted = ODETimeSplitter(self.time_split, self.config['K'])\n",
    "        self.ode_block_1 = ODEBlock(ODEFunction(self.Graph), self.config['solver'], 0, self.odetime_splitted[0])\n",
    "        self.ode_block_2 = ODEBlock(ODEFunction(self.Graph), self.config['solver'], self.odetime_splitted[0], self.odetime_splitted[1])\n",
    "        self.ode_block_3 = ODEBlock(ODEFunction(self.Graph), self.config['solver'], self.odetime_splitted[1], self.odetime_splitted[2])\n",
    "        self.ode_block_4 = ODEBlock(ODEFunction(self.Graph), self.config['solver'], self.odetime_splitted[2], self.config['K'])\n",
    "\n",
    "    def get_time(self):\n",
    "        ode_times=list(self.odetime_1)+ list(self.odetime_2)+ list(self.odetime_3)\n",
    "        return ode_times\n",
    "        \n",
    "    def __dropout_x(self, x, keep_prob):\n",
    "        size = x.size()\n",
    "        index = x.indices().t()\n",
    "        values = x.values()\n",
    "        random_index = torch.rand(len(values)) + keep_prob\n",
    "        random_index = random_index.int().bool()\n",
    "        index = index[random_index]\n",
    "        values = values[random_index]/keep_prob\n",
    "        g = torch.sparse.FloatTensor(index.t(), values, size)\n",
    "        return g\n",
    "    \n",
    "    def __dropout(self, keep_prob):\n",
    "        if self.A_split:\n",
    "            graph = []\n",
    "            for g in self.Graph:\n",
    "                graph.append(self.__dropout_x(g, keep_prob))\n",
    "        else:\n",
    "            graph = self.__dropout_x(self.Graph, keep_prob)\n",
    "        return graph\n",
    "    \n",
    "    def computer(self):\n",
    "        \"\"\"\n",
    "        propagate methods for LT-NCF\n",
    "        \"\"\"       \n",
    "        users_emb = self.embedding_user.weight\n",
    "        items_emb = self.embedding_item.weight\n",
    "        all_emb = torch.cat([users_emb, items_emb])\n",
    "        embs = [all_emb]\n",
    "        \n",
    "        \"\"\"\n",
    "        layers\n",
    "        \"\"\"\n",
    "        all_emb_1 = self.ode_block_1(all_emb)\n",
    "        all_emb_1 = all_emb_1 - all_emb\n",
    "        embs.append(all_emb_1)\n",
    "\n",
    "        all_emb_2 = self.ode_block_2(all_emb_1)\n",
    "        all_emb_2 = all_emb_2 - all_emb_1\n",
    "        embs.append(all_emb_2)\n",
    "\n",
    "        all_emb_3 = self.ode_block_3(all_emb_2)\n",
    "        all_emb_3 = all_emb_3 - all_emb_2\n",
    "        embs.append(all_emb_3)\n",
    "        \n",
    "        all_emb_4 = self.ode_block_4(all_emb_3)\n",
    "        all_emb_4 = all_emb_4 - all_emb_3\n",
    "        embs.append(all_emb_4)\n",
    "\n",
    "        embs = torch.stack(embs, dim=1)\n",
    "        light_out = torch.mean(embs, dim=1)        \n",
    "\n",
    "        users, items = torch.split(light_out, [self.num_users, self.num_items])\n",
    "        return users, items\n",
    "    \n",
    "    def getUsersRating(self, users):\n",
    "        all_users, all_items = self.computer()\n",
    "        users_emb = all_users[users.long()]\n",
    "        items_emb = all_items\n",
    "        rating = self.f(torch.matmul(users_emb, items_emb.t()))\n",
    "        return rating\n",
    "    \n",
    "    def getEmbedding(self, users, pos_items, neg_items):\n",
    "        all_users, all_items = self.computer()\n",
    "        users_emb = all_users[users]\n",
    "        pos_emb = all_items[pos_items]\n",
    "        neg_emb = all_items[neg_items]\n",
    "        users_emb_ego = self.embedding_user(users)\n",
    "        pos_emb_ego = self.embedding_item(pos_items)\n",
    "        neg_emb_ego = self.embedding_item(neg_items)\n",
    "        return users_emb, pos_emb, neg_emb, users_emb_ego, pos_emb_ego, neg_emb_ego\n",
    "    \n",
    "    def bpr_loss(self, users, pos, neg):\n",
    "        (users_emb, pos_emb, neg_emb, \n",
    "        userEmb0,  posEmb0, negEmb0) = self.getEmbedding(users.long(), pos.long(), neg.long())\n",
    "        reg_loss = (1/2)*(userEmb0.norm(2).pow(2) + \n",
    "                         posEmb0.norm(2).pow(2)  +\n",
    "                         negEmb0.norm(2).pow(2))/float(len(users))\n",
    "        pos_scores = torch.mul(users_emb, pos_emb)\n",
    "        pos_scores = torch.sum(pos_scores, dim=1)\n",
    "        neg_scores = torch.mul(users_emb, neg_emb)\n",
    "        neg_scores = torch.sum(neg_scores, dim=1)\n",
    "        \n",
    "        loss = torch.mean(torch.nn.functional.softplus(neg_scores - pos_scores))\n",
    "        \n",
    "        return loss, reg_loss\n",
    "       \n",
    "    def forward(self, users, items):\n",
    "        # compute embedding\n",
    "        all_users, all_items = self.computer()\n",
    "        users_emb = all_users[users]\n",
    "        items_emb = all_items[items]\n",
    "        inner_pro = torch.mul(users_emb, items_emb)\n",
    "        gamma     = torch.sum(inner_pro, dim=1)\n",
    "        return gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;30;43mloading [../data/gowalla]\u001b[0m\n",
      "810128 interactions for training\n",
      "217242 interactions for testing\n",
      "gowalla Sparsity : 0.0008396216228570436\n",
      "gowalla is ready to go\n"
     ]
    }
   ],
   "source": [
    "if world.dataset in ['gowalla', 'yelp2018', 'amazon-book']:\n",
    "    dataset = dataloader.Loader(path=\"../data/\"+world.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;30;43muse NORMAL distribution initilizer\u001b[0m\n",
      "loading adjacency matrix\n",
      "successfully loaded...\n",
      "don't split the matrix\n",
      "lgn is already to go(dropout:0)\n",
      "load and save to /home/bigdyl/jeongwhanchoi/tutorial21/LT-OCF/code/pretrain/ltocf\n"
     ]
    }
   ],
   "source": [
    "# Recmodel = register.MODELS[world.model_name](world.config, dataset)\n",
    "Recmodel = LTOCF(world.config, dataset)\n",
    "\n",
    "Recmodel = Recmodel.to(world.device)\n",
    "if world.config['learnable_time'] == False:\n",
    "    bpr = utils.BPRLoss(Recmodel, world.config)\n",
    "elif world.config['learnable_time'] == True:\n",
    "    bpr_t = utils.BPRLossT(Recmodel, world.config)\n",
    "\n",
    "weight_file = utils.getFileName()\n",
    "pretrained_weight_file = utils.getPretrainedFileName(world.config['pretrained_file_name'])\n",
    "print(f\"load and save to {pretrained_weight_file}\")\n",
    "if world.LOAD:\n",
    "    try:\n",
    "        Recmodel.load_state_dict(torch.load(pretrained_weight_file,map_location=torch.device('cpu')))\n",
    "        world.cprint(f\"loaded model weights from {pretrained_weight_file}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"{pretrained_weight_file} not exists, start from beginning\")\n",
    "Neg_k = 1\n",
    "\n",
    "save_name = time.strftime(\"%m-%d-%Hh%Mm-\") + \"-\" + world.dataset + \"-\" + world.model_name + \"-\" + world.config['solver'] + \"-adjoint_\" + str(world.adjoint) + \"-learnable_t_\" + str(world.config['learnable_time']) + \"-dual_res_\" + str(world.config['dual_res']) + \"-lr\" + str(world.config['lr']) + \"-lr_time\" + str(world.config['lr_time']) + \"-decay\" + str(world.config['decay'])+ \"-\" + world.comment\n",
    "\n",
    "# init tensorboard\n",
    "if world.tensorboard:\n",
    "    w : SummaryWriter = SummaryWriter(join(world.BOARD_PATH, save_name))\n",
    "else:\n",
    "    w = None\n",
    "    world.cprint(\"not enable tensorflowboard\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;30;43m[TEST]\u001b[0m\n",
      "Inference time: 6.2452s\n",
      "{'precision': array([0.00020932]), 'recall': array([0.00060648]), 'ndcg': array([0.00044858])}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_83017/160973371.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mworld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_name\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'ltocf'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mworld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'learnable_time'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m             \u001b[0moutput_information\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mProcedure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBPR_train_original\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRecmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNeg_k\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'EPOCH[{epoch+1}/{world.TRAIN_epochs}] {output_information}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jeongwhanchoi/tutorial21/LT-OCF/code/Procedure.py\u001b[0m in \u001b[0;36mBPR_train_original\u001b[0;34m(dataset, recommend_model, loss_class, epoch, neg_k, w)\u001b[0m\n\u001b[1;32m     39\u001b[0m                                                    \u001b[0mnegItems\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m                                                    batch_size=world.config['bpr_batch_size'])):\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mcri\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbpr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstageOne\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_users\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_neg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0maver_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcri\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mworld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensorboard\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jeongwhanchoi/tutorial21/LT-OCF/code/utils.py\u001b[0m in \u001b[0;36mstageOne\u001b[0;34m(self, users, pos, neg)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tutorial/lib/python3.9/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tutorial/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(world.TRAIN_epochs+1):\n",
    "    start = time.time()\n",
    "    if epoch %10 == 0:\n",
    "        cprint(\"[TEST]\")\n",
    "        Procedure.Test(dataset, Recmodel, epoch, w, world.config['multicore'])\n",
    "\n",
    "    if world.model_name == 'ltocf':\n",
    "        if world.config['learnable_time'] == False:\n",
    "            output_information = Procedure.BPR_train_original(dataset, Recmodel, bpr, epoch, neg_k=Neg_k,w=w)\n",
    "            print(f'EPOCH[{epoch+1}/{world.TRAIN_epochs}] {output_information}')\n",
    "        else:\n",
    "            output_information, times_list= Procedure.BPR_train_ode(dataset, Recmodel, bpr_t, epoch, neg_k=Neg_k,w=w)\n",
    "            print(f'EPOCH[{epoch+1}/{world.TRAIN_epochs}] {output_information}')\n",
    "            print(times_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "██╗  ████████╗    ██████╗  ██████╗███████╗\n",
      "██║  ╚══██╔══╝   ██╔═══██╗██╔════╝██╔════╝\n",
      "██║     ██║█████╗██║   ██║██║     █████╗  \n",
      "██║     ██║╚════╝██║   ██║██║     ██╔══╝  \n",
      "███████╗██║      ╚██████╔╝╚██████╗██║     \n",
      "╚══════╝╚═╝       ╚═════╝  ╚═════╝╚═╝     \n",
      "                                                      \n",
      "\n",
      "Current cuda device  0\n",
      ">>SEED: 2020\n",
      "\u001b[0;30;43mloading [../data/gowalla]\u001b[0m\n",
      "810128 interactions for training\n",
      "217242 interactions for testing\n",
      "gowalla Sparsity : 0.0008396216228570436\n",
      "gowalla is ready to go\n",
      "===========config================\n",
      "{'A_n_fold': 100,\n",
      " 'A_split': False,\n",
      " 'K': 4.0,\n",
      " 'bigdata': False,\n",
      " 'bpr_batch_size': 2048,\n",
      " 'decay': 0.0001,\n",
      " 'dropout': 0,\n",
      " 'dual_res': False,\n",
      " 'keep_prob': 0.6,\n",
      " 'latent_dim_rec': 64,\n",
      " 'learnable_time': False,\n",
      " 'lightGCN_n_layers': 3,\n",
      " 'lr': 0.001,\n",
      " 'lr_time': 0.0001,\n",
      " 'multicore': 0,\n",
      " 'pretrain': 0,\n",
      " 'pretrained_file_name': 'ltocf',\n",
      " 'solver': 'rk4',\n",
      " 'test_u_batch_size': 100,\n",
      " 'time_split': 4}\n",
      "cores for test: 32\n",
      "comment: fixed_time\n",
      "tensorboard: 1\n",
      "LOAD: 0\n",
      "Weight path: ./checkpoints\n",
      "Test Topks: [20]\n",
      "using bpr loss\n",
      "adjoint method: False\n",
      "===========end===================\n",
      "\u001b[0;30;43muse NORMAL distribution initilizer\u001b[0m\n",
      "loading adjacency matrix\n",
      "successfully loaded...\n",
      "don't split the matrix\n",
      "lgn is already to go(dropout:0)\n",
      "load and save to /home/bigdyl/jeongwhanchoi/tutorial21/LT-OCF/code/pretrain/ltocf\n",
      "\u001b[0;30;43m[TEST]\u001b[0m\n",
      "Inference time: 11.9485s\n",
      "{'precision': array([0.00398218]), 'recall': array([0.01336332]), 'ndcg': array([0.01033115])}\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bigdyl/jeongwhanchoi/tutorial21/LT-OCF/code/main.py\", line 56, in <module>\n",
      "    output_information = Procedure.BPR_train_original(dataset, Recmodel, bpr, epoch, neg_k=Neg_k,w=w)\n",
      "  File \"/home/bigdyl/jeongwhanchoi/tutorial21/LT-OCF/code/Procedure.py\", line 41, in BPR_train_original\n",
      "    cri = bpr.stageOne(batch_users, batch_pos, batch_neg)\n",
      "  File \"/home/bigdyl/jeongwhanchoi/tutorial21/LT-OCF/code/utils.py\", line 40, in stageOne\n",
      "    loss.backward()\n",
      "  File \"/home/bigdyl/anaconda3/envs/tutorial/lib/python3.9/site-packages/torch/_tensor.py\", line 255, in backward\n",
      "    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)\n",
      "  File \"/home/bigdyl/anaconda3/envs/tutorial/lib/python3.9/site-packages/torch/autograd/__init__.py\", line 147, in backward\n",
      "    Variable._execution_engine.run_backward(\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python main.py --dataset=\"gowalla\" --model=\"ltocf\" --solver=\"rk4\" --adjoint=False --learnable_time=False --dual_res=False --K=4 --lr=1e-3 --decay=1e-4 --topks=\"[20]\" --comment=\"fixed_time\" --tensorboard=1 --gpuid=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ba9d02783c44b8ec937f23dd852464beb8358e14fc8e094e377d5d0872d3888a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit ('tutorial': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
